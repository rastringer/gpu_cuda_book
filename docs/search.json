[
  {
    "objectID": "gpu_architecture.html",
    "href": "gpu_architecture.html",
    "title": "2  Getting to know the device and platform",
    "section": "",
    "text": "2.1 Abstractions\nCUDA, or “Compute Unite Device Architecture” as it was introduced in 2006, is a parallel computing platform and programming model that uses the parallel engine in NVIDIA GPUs to solve computational tasks.\nThere are three principal abstractions:\nBasically CUDA allows developers to partition problems into sub-problems that can be solved by threads running in parallel, in blocks. Threads all run the same code, and an ID for each thread allows access to memory addresses and control decisions.\nThreads are arranged as a grid of thread blocks.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting to know the device and platform</span>"
    ]
  },
  {
    "objectID": "gpu_architecture.html#abstractions",
    "href": "gpu_architecture.html#abstractions",
    "title": "2  Getting to know the device and platform",
    "section": "",
    "text": "hierarchy of thread groups\nshared memories\nbarrier synchronization",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting to know the device and platform</span>"
    ]
  },
  {
    "objectID": "gpu_architecture.html#ptx",
    "href": "gpu_architecture.html#ptx",
    "title": "2  Getting to know the device and platform",
    "section": "2.2 PTX",
    "text": "2.2 PTX\nPTX is a low-level, parallel thread execution virtual machine and instruction set architecture (ISA). In other words, it is a paradigm that leverages the GPU as a data-parallel computing device.\n\n2.2.1 Programming model\nPTX’s programming model is parallel: it specifies the execution of a given thread of a parallel thread array. A CTA, or cooperative thread array, is an array of threads that execute a kernel concurrently or in parallel.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting to know the device and platform</span>"
    ]
  },
  {
    "objectID": "gpu_architecture.html#sass",
    "href": "gpu_architecture.html#sass",
    "title": "2  Getting to know the device and platform",
    "section": "2.3 SASS",
    "text": "2.3 SASS\nSASS is the low-level assembly language that compiles to binary microcode, which executes natively on NVIDIA GPUs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting to know the device and platform</span>"
    ]
  },
  {
    "objectID": "gpu_architecture.html#high-level-architecture",
    "href": "gpu_architecture.html#high-level-architecture",
    "title": "2  Getting to know the device and platform",
    "section": "2.4 High level architecture",
    "text": "2.4 High level architecture\nGPUs have highly parallel processor architecture, comprising processing elements and memory hierarchy. Streaming processors do work on data, and that data and code are accessed from the high bandwidth memory (HMB3 in the diagram) via the L2 cache.\nThe A100 GPU, for example, has 108 SMs, a 40MB L2 cache, and up to 2039 GB/s bandwidth from 80GB of HBM2 memory.\nNVLink Network Interconnect enables GPU-to-GPU communication among up to 256 GPUs across multiple compute nodes.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting to know the device and platform</span>"
    ]
  },
  {
    "objectID": "gpu_architecture.html#streaming-multiprocessors",
    "href": "gpu_architecture.html#streaming-multiprocessors",
    "title": "2  Getting to know the device and platform",
    "section": "2.5 Streaming multiprocessors",
    "text": "2.5 Streaming multiprocessors\n\nEach Streaming Multiprocessor has a set of execution units, a reguster file and some shared memory.\nWe also notice the warp scheduler - this is a basic unit of execution and a collection of threads. Typically these are groups of 32 threads, which are executed together by a SM.\nTensor Cores are specialized units focused on speeding up deep learning workloads. They accel at mixed-precision matrix multiply and gradient accumulation calculations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting to know the device and platform</span>"
    ]
  },
  {
    "objectID": "gpu_architecture.html#teraflops",
    "href": "gpu_architecture.html#teraflops",
    "title": "2  Getting to know the device and platform",
    "section": "2.6 TeraFlops",
    "text": "2.6 TeraFlops\nIt’s worth familiarizing ourselves with TFLOPS, which stands for Trillion Floating Point Operations Per Second. This is commonly used to measure the performance of GPUs.\n1 TFLOP = 1 trillion floating point calculations per second (what’s a floating point? Just a number with a decimal eg 1.2 or 12.3456)\nThe H100 with SXM% board form-factor can perform 133.8 TFLOPs on FP16 inputs. FP16 just means half precision, or 1 bit for sign (+, -), 5 bits for the exponent, and 10 bits for decimal precision. This is a very popular format for AI training and inference, since a minimal drop in accuracy also means better speed and memory efficiency.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting to know the device and platform</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html",
    "href": "c_to_cuda.html",
    "title": "3  Getting started with CUDA",
    "section": "",
    "text": "3.1 Resources\nWe can do all exercises on the free T4 GPU on Colab.\nLet’s check we have the Nvidia CUDA Compiler Driver (NVCC) installed:\n!nvcc --version\n!pip install nvcc4jupyter\n%load_ext nvcc4jupyter",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#writing-and-running-c-code-in-a-colab",
    "href": "c_to_cuda.html#writing-and-running-c-code-in-a-colab",
    "title": "3  Getting started with CUDA",
    "section": "3.2 Writing and running C code in a Colab",
    "text": "3.2 Writing and running C code in a Colab\nThanks to the nvcc4jupyter extension, we can run C/C++ code from our notebook cells.\nSimply annotate each code cell with %%cuda at the top.\nSyntax checking may mean a lot of read and yellow lines on our code, since it’s focused on Python code, so it’s best to turn this feature off: Settings -&gt; Editor -&gt; Code diagnostics -&gt; None.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#hello-world",
    "href": "c_to_cuda.html#hello-world",
    "title": "3  Getting started with CUDA",
    "section": "3.3 Hello, World!",
    "text": "3.3 Hello, World!\n\n%%cuda\n\n#include &lt;stdio.h&gt;\n\nvoid hello()\n{\n  printf(\"Hello from the CPU.\\n\");\n}\n\nint main()\n{\n  hello();\n  return 0;\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#and-from-the-gpu",
    "href": "c_to_cuda.html#and-from-the-gpu",
    "title": "3  Getting started with CUDA",
    "section": "3.4 and from the GPU",
    "text": "3.4 and from the GPU\nLet’s adjust the code to make it run on the GPU.\nWe will need to annotate functions with\n__global__\nand synchronize our code on the completion of the kernel using the method\ncudaDeviceSynchronize();\n\n%%cuda\n\n#include &lt;stdio.h&gt;\n\n__global__ void helloGPU()\n{\n  printf(\"Hello from the GPU.\\n\");\n}\n\nint main()\n{\n  helloGPU&lt;&lt;&lt;1, 1&gt;&gt;&gt;();\n  cudaDeviceSynchronize();\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#blocks-and-threads",
    "href": "c_to_cuda.html#blocks-and-threads",
    "title": "3  Getting started with CUDA",
    "section": "3.5 Blocks and threads",
    "text": "3.5 Blocks and threads\nYou may be wondering why we have triple angle brackets in the function call:\n&lt;&lt;&lt;1, 1&gt;&gt;&gt;\nThese are required parameters for CUDA denoting the blocks and threads in which our tasks should run.\n&lt;&lt;&lt; NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK&gt;&gt;&gt;\nThreads and blocks are fundamental for organizing parallel computation on GPUs. Threads are the smallest unit of action, each capable of running a single instance of the kernel function.\nThreads are grouped into blocks, where they can cooperate and share resources.\nMultiple blocks form a grid, the highest level of CUDA hierarchy.\nkernelA &lt;&lt;&lt;1, 1&gt;&gt;&gt;() runs one block with a single thread so will run only once.\nkernelB &lt;&lt;&lt;1, 10&gt;&gt;&gt;() runs one block with 10 threads and will run 10 times.\nkernelC &lt;&lt;&lt;10, 1&gt;&gt;&gt;() runs 10 thread blocks, each with a single thread so will run 10 times.\nkernelD &lt;&lt;&lt;10, 10&gt;&gt;&gt;() runs 10 blocks which each have 10 thread, so run 100 times.\nThe next example illustrates the use of threads for parallel action. Experiment with changing the &lt;&lt;&lt;blocks, threads&gt;&gt;&gt; and see the results.\n\n%%cuda\n\n#include &lt;stdio.h&gt;\n\n__global__ void printInts()\n{\n  for(int i=0; i&lt;10; i++)\n  {\n      printf(\"%d \", i);\n  }\n}\n\nint main()\n{\n  printInts&lt;&lt;&lt;1, 1&gt;&gt;&gt;();\n  cudaDeviceSynchronize();\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#threads-and-indexes",
    "href": "c_to_cuda.html#threads-and-indexes",
    "title": "3  Getting started with CUDA",
    "section": "3.6 Threads and indexes",
    "text": "3.6 Threads and indexes\nEach thread has an index denoting its place in a block. Blocks also are indexed, and grouped into a grid.\nThere are useful methods for identifying these indexes:\nthreadIdx.x : identifies the index of the thread blockIdx.x : identifies the index of the block blockDim.x : represents the number of threads in a block\nFor example, here is a classical loop:\n\n%%cuda\n\n#include &lt;stdio.h&gt;\n\nvoid loop(int n)\n{\n    for(int i=0; i&lt;n; i++)\n  {\n      printf(\"This is loop cycle %d \", i);\n  }\n}\n\nint main()\n{\n  loop(10);\n}\n\nWe can accelerate this operation by launching the iterations in parallel, a multi-block loop. Let’s use two blocks of threads.\nHere, blockIdx.x * blockDim.x + threadIdx.x gives threads unique indexes in a grid.\n\n%%cuda\n\n#include &lt;stdio.h&gt;\n\n__global__ void loop()\n{\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  printf(\"This is loop cycle %d\\n\", i);\n}\n\nint main()\n{\n\n  loop&lt;&lt;&lt;2, 5&gt;&gt;&gt;();\n  cudaDeviceSynchronize();\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#allocating-memory",
    "href": "c_to_cuda.html#allocating-memory",
    "title": "3  Getting started with CUDA",
    "section": "3.7 Allocating memory",
    "text": "3.7 Allocating memory\nCUDA version 6 and above has simplified memory allocation for both the CPU host and as or or many GPU devices with little additional work necessary by the developer.\nC uses calls to malloc and free to allocate and liberate memory; we simply replace these with cudaMallocManaged and cudaFree.\n\n%%cuda\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n// CPU-only\n\nint square(int x)\n{\n  return x = x * x;\n}\n\nint main()\n{\n    int N = 100000;\n    size_t size = N * sizeof(int);\n    int *a = (int *)malloc(size);\n\n    // Fill array with numbers 1-20\n    for (int i = 0; i &lt; N; i++) {\n        a[i] = i + 1;\n    }\n\n    // Square each number and print\n    printf(\"Original -&gt; Squared\\n\");\n    for (int i = 0; i &lt; N; i++) {\n        int result = square(a[i]);\n        printf(\"%2d -&gt; %4d\\n\", a[i], result);\n    }\n    free(a);\n    return 0;\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#profiling",
    "href": "c_to_cuda.html#profiling",
    "title": "3  Getting started with CUDA",
    "section": "3.8 Profiling",
    "text": "3.8 Profiling\nLet’s create an executable with the same code so we can profile and time it. To do that, we write a file using cell magic functions, compile and run.\n\n%%writefile square.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n// CPU-only\n\nint square(int x)\n{\n  return x = x * x;\n}\n\nint main()\n{\n    int N = 100000;\n    size_t size = N * sizeof(int);\n    int *a = (int *)malloc(size);\n\n    // Fill array with numbers 1-20\n    for (int i = 0; i &lt; N; i++) {\n        a[i] = i + 1;\n    }\n\n    // Square each number and print\n    printf(\"Original -&gt; Squared\\n\");\n    for (int i = 0; i &lt; N; i++) {\n        int result = square(a[i]);\n        printf(\"%2d -&gt; %4d\\n\", a[i], result);\n    }\n    free(a);\n    return 0;\n}\n\n\n!gcc square.c -o square",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#time-it",
    "href": "c_to_cuda.html#time-it",
    "title": "3  Getting started with CUDA",
    "section": "3.9 Time it",
    "text": "3.9 Time it\n\n!time ./square",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#exercise-cuda-version",
    "href": "c_to_cuda.html#exercise-cuda-version",
    "title": "3  Getting started with CUDA",
    "section": "3.10 Exercise: CUDA version",
    "text": "3.10 Exercise: CUDA version\nLet’s use cudaMallocManaged and cudaFree. Developers often prefix variables to be put on the device with d_, we will write device_ to make this clear.\n\n%%cuda\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n// Accelerated\n\nint N = 100000;\nsize_t size = N * sizeof(int);\n\nint *device_a;\n\n/*\n Here is our earlier square function:\n int square(int x)\n{\n  return x = x * x;\n}\n */\n\n// Initialize the array\nvoid init(int *a, int N)\n{\n  int i;\n  for (i = 0; i &lt; N; ++i)\n  {\n    a[i] = i;\n  }\n}\n\n/*\n Here is our earlier square function:\n int square(int x)\n{\n  return x = x * x;\n}\nHow can we make this into a CUDA function?\nRemember the threadIdx.x etc methods to\ncreate a unique index for each thread across all blocks\n */\n\n__global__ void square_kernel(int *device_a, int n)\n{\n    // initialize an index variable\n    // Your code here;\n    if (idx &lt; n) {\n        // square the device array at the index\n        // Your code here\n    }\n}\n\n// Use `a` on the CPU and/or on any GPU in the accelerated system.\n\nint main() {\n    // Use cudaMallocManaged to allocate memory for the device array\n    // and the size variable\n    // Your code here\n\n    // Initialize the array\n    for (int i = 0; i &lt; N; i++) {\n        device_a[i] = i + 1;  // Values will be 1, 2, 3, ..., 10\n    }\n\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n    square_kernel&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(device_a, N);\n\n    cudaDeviceSynchronize();\n\n    printf(\"Squared array:\\n\");\n    for (int i = 0; i &lt; N; i++) {\n        printf(\"%d \", device_a[i]);\n    }\n\n    cudaFree(device_a);\n\n    return 0;\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#solution",
    "href": "c_to_cuda.html#solution",
    "title": "3  Getting started with CUDA",
    "section": "3.11 Solution",
    "text": "3.11 Solution\n\n%%cuda\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n// Accelerated\n\nint N = 100000;\nsize_t size = N * sizeof(int);\n\nint *device_a;\n\n__global__ void square_kernel(int *device_a, int n)\n{\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx &lt; n) {\n        device_a[idx] = device_a[idx] * device_a[idx];\n    }\n}\n\n// Use `a` on the CPU and/or on any GPU in the accelerated system.\n\nint main() {\n    // Note the address of `a` is passed as first argument.\n    cudaMallocManaged(&device_a, size);\n\n    // Initialize the array\n    for (int i = 0; i &lt; N; i++) {\n        device_a[i] = i + 1;  // Values will be 1, 2, 3, ..., 10\n    }\n\n    int threadsPerBlock = 8;\n    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n    square_kernel&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(device_a, N);\n\n    cudaDeviceSynchronize();\n\n    printf(\"Squared array:\\n\");\n    for (int i = 0; i &lt; N; i++) {\n        printf(\"%d \", device_a[i]);\n    }\n\n    cudaFree(device_a);\n\n    return 0;\n}\n\n\nLet’s take the same approach to time our CUDA equivalent square function.\n\n%%writefile square.cu\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\n// Accelerated\n\nint N = 100000;\nsize_t size = N * sizeof(int);\n\nint *device_a;\n\n__global__ void square_kernel(int *device_a, int n)\n{\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx &lt; n) {\n        device_a[idx] = device_a[idx] * device_a[idx];\n    }\n}\n\n// Use `a` on the CPU and/or on any GPU in the accelerated system.\n\nint main() {\n    // Note the address of `a` is passed as first argument.\n    cudaMallocManaged(&device_a, size);\n\n    // Initialize the array\n    for (int i = 0; i &lt; N; i++) {\n        device_a[i] = i + 1;  // Values will be 1, 2, 3, ..., 10\n    }\n\n    int threadsPerBlock = 8;\n    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n    square_kernel&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(device_a, N);\n\n    cudaDeviceSynchronize();\n\n    printf(\"Squared array:\\n\");\n    for (int i = 0; i &lt; N; i++) {\n        printf(\"%d \", device_a[i]);\n    }\n\n    cudaFree(device_a);\n\n    return 0;\n}\n\n\n!nvcc -o cuda_square square.cu\n\n\n!nvprof ./cuda_square\n\n\n!time ./cuda_square",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#grid-stride",
    "href": "c_to_cuda.html#grid-stride",
    "title": "3  Getting started with CUDA",
    "section": "3.12 Grid stride",
    "text": "3.12 Grid stride\nLet’s remind ourselves: grids are the highest level of the GPU hierarchy:\nThreads -&gt; Blocks -&gt; Grids\nOften in CUDA programming, the number of threads in a grid is smaller than the data upon which we operate.\nIf we have an array of 100 elements to do something with, and a grid of 25 threads, each grid will have to perform computation 4 times.\nA thread at index 20 in the grid would:\n\nPerform its operation (eg squaring the element) on element 20 of the array\nIncrement its index by 25, the size of the grid, to 45\nPerform its operation on element 45 of the array\nIncrement its index by 25, to 70\nPerform its operation on element 70 of the array\nIncrement its index by 25, to 95\nPerform its operation on element 95 of the array\nStop its work, since 120 is out of range for the array\n\nWe can use the CUDA variable gridDim.x to calculate the number of blocks in the grid. Then we calculate the number of threads in each block, using\ngridDim.x * blockDim.x\nHere is a grid stride loop in a kernel:\n__global__ void kernel(int *a, int N)\n{\n  int indexInGrid = threadIdx.x + blockIdx.x * blockDim.x;\n  int gridStride = gridDim.x * blockDim.x;\n\n  for (int i = indexInGrid; i &lt; N; i += gridStride)\n  {\n    // do something to a[i];\n  }\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#exercise",
    "href": "c_to_cuda.html#exercise",
    "title": "3  Getting started with CUDA",
    "section": "3.13 Exercise",
    "text": "3.13 Exercise\nFor this exercise, we use the %%writefile magic function to create a file so we can then generate an outputs for a visualization, rather than running it inline with %%cuda.\n\n%%writefile grid_stride.cu\n\n#include &lt;stdio.h&gt;\n\nvoid init(int *a, int N)\n{\n  int i;\n  for (i = 0; i &lt; N; ++i)\n  {\n    a[i] = i;\n  }\n}\n\n__global__\nvoid squareElements(int *a, int N)\n{\n\n  /*\n   * Use a grid-stride loop to ensure each thread works\n   * on more than one array element\n   */\n\n  int idx = // Your code here\n  int stride = // Your code here\n\n  for (int i = idx; i &lt; N; i += stride)\n  {\n    int old_value = a[i];\n    a[i] *= i;\n    printf(\"Thread %d (Block %d, Thread in Block %d) processing element %d. Old value: %d, New value: %d\\n\",\n           idx, blockIdx.x, threadIdx.x, i, old_value, a[i]);\n  }\n}\n\nbool checkElementsSquared(int *a, int N)\n{\n  int i;\n  for (i = 0; i &lt; N; ++i)\n  {\n    if (a[i] != i*i) return false;\n  }\n  return true;\n}\n\nint main()\n{\n  int N = 500;\n  int *a;\n\n  size_t size = N * sizeof(int);\n\n  // Use cudaMallocManaged to allocate memory for the array\n  // and the size variable\n  // Your code her\n\n  init(a, N);\n\n  // The size of this grid is 356 (32 x 8)\n  size_t threads_per_block = 32;\n  size_t number_of_blocks = 8;\n\n  squareElements&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;(a, N);\n  cudaDeviceSynchronize();\n\n  bool areSquared = checkElementsSquared(a, N);\n  printf(\"All elements were doubled? %s\\n\", areSquared ? \"TRUE\" : \"FALSE\");\n\n  cudaFree(a);\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#solution-1",
    "href": "c_to_cuda.html#solution-1",
    "title": "3  Getting started with CUDA",
    "section": "3.14 Solution",
    "text": "3.14 Solution\n\n%%writefile grid_stride.cu\n\n#include &lt;stdio.h&gt;\n\nvoid init(int *a, int N)\n{\n  int i;\n  for (i = 0; i &lt; N; ++i)\n  {\n    a[i] = i;\n  }\n}\n\n__global__\nvoid squareElements(int *a, int N)\n{\n\n  /*\n   * Use a grid-stride loop to ensure each thread works\n   * on more than one array element\n   */\n\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = gridDim.x * blockDim.x;\n\n  for (int i = idx; i &lt; N; i += stride)\n  {\n    int old_value = a[i];\n    a[i] *= i;\n    printf(\"Thread %d (Block %d, Thread in Block %d) processing element %d. Old value: %d, New value: %d\\n\",\n           idx, blockIdx.x, threadIdx.x, i, old_value, a[i]);\n  }\n}\n\nbool checkElementsSquared(int *a, int N)\n{\n  int i;\n  for (i = 0; i &lt; N; ++i)\n  {\n    if (a[i] != i*i) return false;\n  }\n  return true;\n}\n\nint main()\n{\n  int N = 500;\n  int *a;\n\n  size_t size = N * sizeof(int);\n  cudaMallocManaged(&a, size);\n\n  init(a, N);\n\n  // The size of this grid is 356 (32 x 8)\n  size_t threads_per_block = 32;\n  size_t number_of_blocks = 8;\n\n  squareElements&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;(a, N);\n  cudaDeviceSynchronize();\n\n  bool areSquared = checkElementsSquared(a, N);\n  printf(\"All elements were doubled? %s\\n\", areSquared ? \"TRUE\" : \"FALSE\");\n\n  cudaFree(a);\n}\n\n\n!nvcc -o grid_stride grid_stride.cu\n!./grid_stride &gt; output.txt",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "c_to_cuda.html#visualizing-the-grid-stride",
    "href": "c_to_cuda.html#visualizing-the-grid-stride",
    "title": "3  Getting started with CUDA",
    "section": "3.15 Visualizing the grid stride",
    "text": "3.15 Visualizing the grid stride\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom google.colab import files\n%matplotlib inline\n\ndef parse_line(line):\n    pattern = r\"Thread (\\d+) \\(Block (\\d+), Thread in Block (\\d+)\\) processing element (\\d+)\\. Old value: (\\d+), New value: (\\d+)\"\n    match = re.search(pattern, line)\n    if match:\n        return {\n            'Thread ID': int(match.group(1)),\n            'Block ID': int(match.group(2)),\n            'Thread in Block': int(match.group(3)),\n            'Element Index': int(match.group(4)),\n            'Old Value': int(match.group(5)),\n            'New Value': int(match.group(6))\n        }\n    return None\n\n# Read the output file and parse it into a DataFrame\ndata = []\nprint(\"Reading file...\")\nwith open('output.txt', 'r') as f:\n    content = f.read()\n    print(f\"File content (first 500 characters):\\n{content[:500]}\")\n\n    # Use regex to find all matches in the entire content\n    pattern = r\"Thread (\\d+) \\(Block (\\d+), Thread in Block (\\d+)\\) processing element (\\d+)\\. Old value: (\\d+), New value: (\\d+)\"\n    matches = re.finditer(pattern, content)\n\n    for i, match in enumerate(matches, 1):\n        data.append({\n            'Thread ID': int(match.group(1)),\n            'Block ID': int(match.group(2)),\n            'Thread in Block': int(match.group(3)),\n            'Element Index': int(match.group(4)),\n            'Old Value': int(match.group(5)),\n            'New Value': int(match.group(6))\n        })\n        if i % 100 == 0:\n            print(f\"Processed {i} matches...\")\n\nprint(f\"Number of parsed data points: {len(data)}\")\n\ndf = pd.DataFrame(data)\n\n# Print DataFrame info for debugging\nprint(\"\\nDataFrame Info:\")\nprint(df.info())\n\nprint(\"\\nFirst few rows of the DataFrame:\")\nprint(df.head())\n\nif df.empty:\n    print(\"The DataFrame is empty. No visualizations will be created.\")\nelse:\n    # Create a scatter plot\n    plt.figure(figsize=(12, 8))\n    sns.scatterplot(data=df, x='Element Index', y='Thread ID', hue='Block ID', palette='viridis', s=50)\n    plt.title('Grid Stride Pattern Visualization')\n    plt.xlabel('Array Element Index')\n    plt.ylabel('Thread ID')\n    plt.legend(title='Block ID', bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.tight_layout()\n    plt.show()\n\n    # Create a heatmap to show the distribution of work across threads and blocks\n    plt.figure(figsize=(12, 8))\n    heatmap_data = df.pivot_table(values='Element Index', index='Block ID', columns='Thread in Block', aggfunc='count')\n    sns.heatmap(heatmap_data, cmap='YlOrRd', annot=True, fmt='d', cbar_kws={'label': 'Number of Elements Processed'})\n    plt.title('Distribution of Work Across Threads and Blocks')\n    plt.xlabel('Thread in Block')\n    plt.ylabel('Block ID')\n    plt.tight_layout()\n    plt.show()\n\n    # Calculate and print the stride\n    stride = df.groupby('Thread ID')['Element Index'].diff().dropna().mode().iloc[0]\n    print(f\"\\nStride (most common difference between consecutive elements for a thread): {stride}\")\n\nprint(\"Script execution completed.\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting started with CUDA</span>"
    ]
  },
  {
    "objectID": "python_cuda.html",
    "href": "python_cuda.html",
    "title": "4  Python and CUDA",
    "section": "",
    "text": "4.1 Mandelbrot set\nThe Mandelbrot set is a two-dimensional, geometric representation of a fractal and can create visually stunning images.\nIn the following code, we see\nThese lines map pixel coordinates to a point in a complex plane.\nWe iterate using the function f(z) = z^2 + c, where z and c are complex numbers.\nBy checking if the absolute value of z is greater than 2. If |z| &gt; 2, we know it will escape to infinity.\nWith, x_new = x*x - y*y + c_real we calculate the real part of z^2 + c. For a complex number z = a + bi, the real part of z^2 is a^2 - b^2.\ny = 2*x*y + c_imag: This calculates the imaginary part of z^2 + c.\nThe imaginary part of z^2 is 2ab.\nConvergence Test:\nThe Mandelbrot set is defined as the set of points c in the complex plane for which the function f(z) = z^2 + c does not diverge when iterated from z = 0. In practice, we use a finite number of iterations (max_iters) and check if |z| &lt;=\nIf the iteration reaches max_iters without |z| exceeding 2, we consider the point to be in the Mandelbrot set. The number of iterations before |z| &gt; 2 determines the color of points outside the set, creating the characteristic fractal patterns.\noutput[row, col] = i stores the number of iterations it took for the point to escape – this value is used for coloring the plot.\nThe Mandelbrot set can be useful for learning CUDA concepts since we can parallelize across threads and use memory access patterns, as well as GPU-CPU interaction. The calculations are done on the GPU, then sent back to the CPU for the display plot.\nimport numpy as np\nfrom numba import cuda\nimport matplotlib.pyplot as plt\n\n# CUDA kernel function\n@cuda.jit\ndef mandelbrot_kernel(min_x, max_x, min_y, max_y, width, height, max_iters, output):\n    # Get the 2D thread position within the grid\n    row, col = cuda.grid(2)\n\n    # Check if the thread is within the image bounds\n    if row &lt; height and col &lt; width:\n        c_real = min_x + (max_x - min_x) * col / width\n        c_imag = min_y + (max_y - min_y) * row / height\n\n        # Mandelbrot set iteration\n        x = 0\n        y = 0\n        for i in range(max_iters):\n            # Check if the point has escaped\n            if x*x + y*y &gt; 4.0:\n                break\n            # Update x and y\n            x_new = x*x - y*y + c_real\n            y = 2*x*y + c_imag\n            x = x_new\n\n        # Store the number of iterations in the output array\n        output[row, col] = i\n\n# Set up and launch kernel\ndef plot_mandelbrot(min_x, max_x, min_y, max_y, width, height, max_iters):\n    # Output array\n    output = np.zeros((height, width), dtype=np.uint32)\n    # Thread block and grid dimensions\n    threads_per_block = (16, 16)\n    blocks_per_grid = ((width + threads_per_block[0] - 1) // threads_per_block[0],\n                       (height + threads_per_block[1] - 1) // threads_per_block[1])\n\n    # Launch kernel\n    mandelbrot_kernel[blocks_per_grid, threads_per_block](\n        min_x, max_x, min_y, max_y, width, height, max_iters, output\n    )\n\n    # Plot image\n    plt.imshow(output, cmap='hot', extent=[min_x, max_x, min_y, max_y])\n    plt.colorbar()\n    plt.title('Mandelbrot Set')\n    plt.show()\n\n# Example usage\nplot_mandelbrot(-2, 1, -1.5, 1.5, 1000, 1000, 100)\n\n# Additional puzzle: Modify the code to zoom into an interesting part of the Mandelbrot set\n# Hint: Adjust the min_x, max_x, min_y, max_y parameters\n\n/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py:888: NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n  warn(NumbaPerformanceWarning(msg))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python and CUDA</span>"
    ]
  },
  {
    "objectID": "python_cuda.html#mandelbrot-set",
    "href": "python_cuda.html#mandelbrot-set",
    "title": "4  Python and CUDA",
    "section": "",
    "text": "c_real = min_x + (max_x - min_x) * col / width\nc_imag = min_y + (max_y - min_y) * row / height\n\n\nx = 0  # Real part of z\ny = 0  # Imaginary part of z\nfor i in range(max_iters):\n    if x*x + y*y &gt; 4.0:\n        break\n    x_new = x*x - y*y + c_real\n    y = 2*x*y + c_imag\n    x = x_new",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python and CUDA</span>"
    ]
  },
  {
    "objectID": "nsight_attention.html",
    "href": "nsight_attention.html",
    "title": "5  Profiling and optimizing PyTorch training",
    "section": "",
    "text": "!pip install jupyterlab-nvidia-nsight\n\n(Make sure you are using the free T4 runtime in Colab)\nSince using GPUs is the most expensive step in ML training and inference, no small amount of work goes into optimizing their use. In the real world, very few organizations and developers work on low-level kernel optimizations. They typically work further up the stack with frameworks such as PyTorch, leaving PyTorch’s optimizations to those working on its backend (which of course uses CUDA).\nTo give us a lens into the operations being performed on the accelerator and their efficiency in this scenario, there are a variety of profiling tools available. In this notebook, we will explore the use of Nvidia’s Nsight. The software is available as a desktop application and command line tool.\n\n5.0.1 Install Nsight tools\nSince Colabs are essentially a linux-based virtual machine, we can use apt get to install the Nvidia tools\n\n%%bash\n\napt update\napt install -y --no-install-recommends gnupg\necho \"deb http://developer.download.nvidia.com/devtools/repos/ubuntu$(source /etc/lsb-release; echo \"$DISTRIB_RELEASE\" | tr -d .)/$(dpkg --print-architecture) /\" | tee /etc/apt/sources.list.d/nvidia-devtools.list\napt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\napt update\napt install nsight-systems-cli\n\n\n\n5.0.2 Check the installation\n\n!nsys status -e\n\nTimestamp counter supported: Yes\n\nCPU Profiling Environment Check\nRoot privilege: enabled\nLinux Kernel Paranoid Level = 2\nLinux Distribution = Ubuntu\nLinux Kernel Version = 6.1.85+: OK\nLinux perf_event_open syscall available: OK\nSampling trigger event available: OK\nIntel(c) Last Branch Record support: Not Available\nCPU Profiling Environment (process-tree): OK\nCPU Profiling Environment (system-wide): OK\n\nSee the product documentation at https://docs.nvidia.com/nsight-systems for more information,\nincluding information on how to set the Linux Kernel Paranoid Level.\n\n\n\n\n5.0.3 Simple attention\nHere’s our basic attention mechanism that computes query, key, and value matrices to generate weighted representations of input data. The SimpleTransformer class combines this attention mechanism with layer normalization in a residual connection setup.\nWe will include profiling code to measure CPU and GPU performance metrics when running the model on sample input data.\n\n%%writefile profiler.py\n\nimport torch\nimport torch.nn as nn\n\nclass SimpleAttention(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.query = nn.Linear(embed_dim, embed_dim)\n        self.key = nn.Linear(embed_dim, embed_dim)\n        self.value = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, x):\n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n\n        attn_weights = torch.matmul(q, k.transpose(-2, -1))\n        attn_weights = torch.softmax(attn_weights, dim=-1)\n\n        return torch.matmul(attn_weights, v)\n\nclass SimpleTransformer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attention = SimpleAttention(embed_dim)\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        attn_output = self.attention(x)\n        return self.norm(x + attn_output)\n\n# Create a model and sample input\nembed_dim = 256\nseq_length = 100\nbatch_size = 32\n\nmodel = SimpleTransformer(embed_dim, num_heads=1).cuda()\nsample_input = torch.randn(batch_size, seq_length, embed_dim).cuda()\n\nimport torch.cuda.profiler as profiler\n\n# Warm-up run\nmodel(sample_input)\n\n# Profile the model\nwith profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA], record_shapes=True, profile_memory=True, with_stack=True) as prof:\n    with profiler.record_function(\"model_inference\"):\n        model(sample_input)\n\n# Print profiling results\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n\nWriting profiler.py\n\n\n\n!nsys profile --stats=true python profiler.py\n\nCollecting data...\nTraceback (most recent call last):\n  File \"/content/profiler.py\", line 46, in &lt;module&gt;\n    with profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA], record_shapes=True, profile_memory=True, with_stack=True) as prof:\nAttributeError: module 'torch.cuda.profiler' has no attribute 'ProfilerActivity'\nGenerating '/tmp/nsys-report-4b28.qdstrm'\n[1/8] [========================100%] report1.nsys-rep\n[2/8] [========================100%] report1.sqlite\n[3/8] Executing 'nvtx_sum' stats report\nSKIPPED: /content/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n[4/8] Executing 'osrt_sum' stats report\n\n Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)    Min (ns)    Max (ns)    StdDev (ns)            Name         \n --------  ---------------  ---------  ------------  ------------  ---------  -----------  ------------  ----------------------\n     76.0    1,702,896,538         29  58,720,570.3  77,869,201.0      4,006  100,150,826  44,495,375.6  poll                  \n     13.2      295,663,488      1,672     176,832.2       3,329.0      1,004   15,374,049     641,970.2  read                  \n      4.0       89,772,667        611     146,927.4      16,308.0      1,509   23,241,632   1,163,272.8  ioctl                 \n      3.9       86,647,314      5,802      14,934.0       3,006.5      1,006   13,686,743     246,663.1  stat64                \n      1.7       38,487,406        992      38,797.8      13,551.0      2,049      844,849     101,781.4  open64                \n      0.5       10,831,409      7,221       1,500.0       1,469.0      1,000       18,716         610.5  lstat64               \n      0.3        6,090,781          1   6,090,781.0   6,090,781.0  6,090,781    6,090,781           0.0  nanosleep             \n      0.2        3,767,157         76      49,567.9      10,444.5      3,410    2,163,110     247,130.7  mmap64                \n      0.2        3,716,694      1,853       2,005.8       1,794.0      1,018       25,280       1,277.1  fstat64               \n      0.0          596,671          4     149,167.8      49,473.0     35,192      462,533     209,267.0  sem_timedwait         \n      0.0          521,407         74       7,046.0       3,520.5      1,697      169,539      19,585.7  fopen                 \n      0.0          333,517         20      16,675.8       9,374.5      2,751      105,415      22,298.1  mmap                  \n      0.0          303,794         16      18,987.1      12,165.5      1,402       57,334      18,133.4  write                 \n      0.0          261,847          8      32,730.9      32,683.5     23,171       39,984       5,418.0  fgets                 \n      0.0          207,125          3      69,041.7      68,447.0     66,655       72,023       2,733.0  sleep                 \n      0.0          142,461         70       2,035.2       1,419.5      1,079       13,265       1,673.8  fclose                \n      0.0          118,808          2      59,404.0      59,404.0     56,211       62,597       4,515.6  pthread_create        \n      0.0          114,648          9      12,738.7      13,780.0      7,881       21,544       4,325.4  munmap                \n      0.0           92,736         12       7,728.0       4,024.0      1,623       27,163       8,106.5  pthread_cond_signal   \n      0.0           85,302         15       5,686.8       4,267.0      2,028       22,918       5,157.5  open                  \n      0.0           33,190          5       6,638.0       5,099.0      1,635       16,932       5,952.6  fopen64               \n      0.0           23,712          2      11,856.0      11,856.0      8,025       15,687       5,417.9  socket                \n      0.0           10,493          1      10,493.0      10,493.0     10,493       10,493           0.0  connect               \n      0.0            8,915          1       8,915.0       8,915.0      8,915        8,915           0.0  pthread_cond_broadcast\n      0.0            7,372          1       7,372.0       7,372.0      7,372        7,372           0.0  pipe2                 \n      0.0            6,383          1       6,383.0       6,383.0      6,383        6,383           0.0  getc                  \n      0.0            5,305          1       5,305.0       5,305.0      5,305        5,305           0.0  fread                 \n      0.0            5,009          2       2,504.5       2,504.5      1,829        3,180         955.3  sigaction             \n      0.0            4,778          4       1,194.5       1,195.0      1,043        1,345         123.3  fflush                \n      0.0            3,201          3       1,067.0       1,055.0      1,033        1,113          41.3  fcntl                 \n      0.0            2,580          1       2,580.0       2,580.0      2,580        2,580           0.0  fputs_unlocked        \n      0.0            2,300          1       2,300.0       2,300.0      2,300        2,300           0.0  bind                  \n      0.0            1,609          1       1,609.0       1,609.0      1,609        1,609           0.0  fcntl64               \n      0.0            1,505          1       1,505.0       1,505.0      1,505        1,505           0.0  listen                \n      0.0            1,318          1       1,318.0       1,318.0      1,318        1,318           0.0  pthread_mutex_trylock \n\n[5/8] Executing 'cuda_api_sum' stats report\n\n Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)    Min (ns)    Max (ns)   StdDev (ns)               Name            \n --------  ---------------  ---------  ------------  ------------  ---------  ----------  ------------  ----------------------------\n     59.0      133,942,988          8  16,742,873.5      44,480.0     18,304  75,424,540  29,365,709.9  cudaLaunchKernel            \n     19.9       45,123,527          9   5,013,725.2      21,450.0      7,039  44,149,269  14,677,890.0  cudaMemcpyAsync             \n     19.1       43,452,758          2  21,726,379.0  21,726,379.0  5,346,866  38,105,892  23,164,129.4  cudaFree                    \n      0.8        1,898,877          6     316,479.5     216,731.0     12,598     976,777     341,108.4  cudaMalloc                  \n      0.6        1,421,966         18      78,998.1         668.0        616   1,401,568     330,071.6  cudaEventCreateWithFlags    \n      0.3          658,905          3     219,635.0       3,212.0      2,603     653,090     375,383.2  cudaStreamIsCapturing_v10000\n      0.2          469,330      1,149         408.5         256.0        126     146,038       4,301.4  cuGetProcAddress_v2         \n      0.1          182,122          9      20,235.8       7,191.0      5,845      68,490      20,841.1  cudaStreamSynchronize       \n      0.0            7,048          3       2,349.3       2,324.0      2,234       2,490         129.9  cuInit                      \n      0.0            1,790          4         447.5         300.5        252         937         327.1  cuModuleGetLoadingMode      \n\n[6/8] Executing 'cuda_gpu_kern_sum' stats report\n\n Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                                                  Name                                                \n --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  ----------------------------------------------------------------------------------------------------\n     61.8          632,402          3  210,800.7  210,491.0   209,947   211,964      1,043.5  volta_sgemm_128x64_tn                                                                               \n     14.3          146,205          1  146,205.0  146,205.0   146,205   146,205          0.0  volta_sgemm_64x64_tn                                                                                \n     11.2          114,973          1  114,973.0  114,973.0   114,973   114,973          0.0  volta_sgemm_128x64_nn                                                                               \n      8.1           82,590          1   82,590.0   82,590.0    82,590    82,590          0.0  void at::native::&lt;unnamed&gt;::vectorized_layer_norm_kernel&lt;float, float&gt;(int, T2, const T1 *, const T…\n      3.1           32,127          1   32,127.0   32,127.0    32,127    32,127          0.0  void at::native::vectorized_elementwise_kernel&lt;(int)4, at::native::CUDAFunctor_add&lt;float&gt;, at::deta…\n      1.4           14,240          1   14,240.0   14,240.0    14,240    14,240          0.0  void &lt;unnamed&gt;::softmax_warp_forward&lt;float, float, float, (int)7, (bool)0, (bool)0&gt;(T2 *, const T1 …\n\n[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n\n Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          \n --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------\n    100.0          675,793      9  75,088.1     768.0       735   599,634    197,031.0  [CUDA memcpy Host-to-Device]\n\n[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n\n Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n      4.068      9     0.452     0.001     0.001     3.277        1.067  [CUDA memcpy Host-to-Device]\n\nGenerated:\n    /content/report1.nsys-rep\n    /content/report1.sqlite\n\n\n(Numbers will differ slightly each time we run these cells)\nLet’s analyze the “cuda_gpu_kern_sum” report, which shows the GPU kernel executions:\n\nvolta_sgemm_128x64_tn (61.9% of GPU time): This is likely the matrix multiplication for computing attention weights (q * k.transpose(-2, -1)). It’s using NVIDIA’s optimized GEMM (General Matrix Multiplication) kernel. Typically the most compute-intensive operation in a. transformer model.\nvolta_sgemm_64x64_tn (14.2% of GPU time): This could be another part of the attention computation, possibly the final matrix multiplication with the value matrix (attn_weights * v).\nvolta_sgemm_128x64_nn (11.3% of GPU time): This might be the matrix multiplication in one of the linear layers (query, key, or value projection).\nvectorized_layer_norm_kernel (8.1% of GPU time): This corresponds to the LayerNorm operation in the SimpleTransformer class. vectorized_elementwise_kernel (3.1% of GPU time): This could be the element-wise addition in the residual connection (x + attn_output).\nsoftmax_warp_forward (1.4% of GPU time): This is the softmax operation applied to the attention weights.\n\nThe SimpleAttention class operations are primarily represented by items 1, 2, 3, and 6 in this list. These operations account for about 88.8% of the GPU kernel execution time, which indicates that the attention mechanism is indeed a significant part of the computation. To optimize this, we could:\n\nUse the optimized attention mechanism as suggested in the tutorial (torch.nn.functional.scaled_dot_product_attention).\nExperiment with different batch sizes or sequence lengths to find the optimal configuration for your hardware.\nConsider using mixed precision (float16).\n\nLet’s optimize our attention mechanism to use the torch.nn.functional.scaled_dot_product_attention function, optimized for GPUs. This method uses the Flash Attention algorithm when available.\n\n%%writefile profiler.py\n\nimport torch\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nclass OptimizedAttention(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.query = nn.Linear(embed_dim, embed_dim)\n        self.key = nn.Linear(embed_dim, embed_dim)\n        self.value = nn.Linear(embed_dim, embed_dim)\n        self.scale = embed_dim ** -0.5\n\n    def forward(self, x):\n        q = self.query(x)\n        k = self.key(x)\n        v = self.value(x)\n\n        return F.scaled_dot_product_attention(q, k, v, scale=self.scale)\n\n# Update the SimpleTransformer class to use OptimizedAttention\nclass OptimizedTransformer(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attention = OptimizedAttention(embed_dim)\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        attn_output = self.attention(x)\n        return self.norm(x + attn_output)\n\n# Create a model and sample input\nembed_dim = 256\nseq_length = 1000\nbatch_size = 32\n\n# Create a new model with the optimized attention\noptimized_model = OptimizedTransformer(embed_dim, num_heads=1).cuda()\nsample_input = torch.randn(batch_size, seq_length, embed_dim).cuda()\n\nimport torch.cuda.profiler as profiler\n\n# Warm-up run\noptimized_model(sample_input)\n\n# Profile the optimized model\nwith profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA], record_shapes=True, profile_memory=True, with_stack=True) as prof:\n    with profiler.record_function(\"optimized_model_inference\"):\n        optimized_model(sample_input)\n\n# Print profiling results\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n\nOverwriting profiler.py\n\n\n\n!nsys profile --stats=true python profiler.py\n\nCollecting data...\nTraceback (most recent call last):\n  File \"/content/profiler.py\", line 48, in &lt;module&gt;\n    with profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA], record_shapes=True, profile_memory=True, with_stack=True) as prof:\nAttributeError: module 'torch.cuda.profiler' has no attribute 'ProfilerActivity'\nGenerating '/tmp/nsys-report-ed31.qdstrm'\n[1/8] [========================100%] report2.nsys-rep\n[2/8] [========================100%] report2.sqlite\n[3/8] Executing 'nvtx_sum' stats report\nSKIPPED: /content/report2.sqlite does not contain NV Tools Extension (NVTX) data.\n[4/8] Executing 'osrt_sum' stats report\n\n Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)    Max (ns)    StdDev (ns)           Name         \n --------  ---------------  ---------  ------------  -----------  ---------  -----------  ------------  ---------------------\n     79.1      501,166,791         17  29,480,399.5  2,985,176.0      3,734  100,157,864  38,521,524.2  poll                 \n     11.6       73,748,312        639     115,412.1     12,665.0      1,374   18,555,420   1,004,303.4  ioctl                \n      2.9       18,137,686      5,796       3,129.3      2,490.0      1,000       47,046       2,214.1  stat64               \n      1.6        9,881,832      6,366       1,552.3      1,470.0      1,000       21,190         685.9  lstat64              \n      1.3        8,400,908      1,104       7,609.5      2,719.5      1,007      251,640      16,190.3  read                 \n      1.2        7,379,916        992       7,439.4      6,844.5      1,883       28,734       2,950.8  open64               \n      0.8        5,069,938          1   5,069,938.0  5,069,938.0  5,069,938    5,069,938           0.0  nanosleep            \n      0.6        3,616,562         76      47,586.3     10,088.5      4,364    2,222,049     254,372.7  mmap64               \n      0.5        3,406,279      1,852       1,839.2      1,636.5      1,070       49,459       1,738.7  fstat64              \n      0.1          906,478          9     100,719.8     67,874.0     41,581      396,079     111,299.9  sem_timedwait        \n      0.1          354,877         74       4,795.6      3,567.5      1,586       25,689       4,162.3  fopen                \n      0.0          287,427         27      10,645.4      7,683.0      2,462       61,914      11,272.9  mmap                 \n      0.0          252,649          8      31,581.1     32,863.0     12,821       39,628       8,314.4  fgets                \n      0.0          221,124          3      73,708.0     71,618.0     68,171       81,335       6,826.3  sleep                \n      0.0          157,808         16       9,863.0      8,608.0      4,446       27,809       5,358.3  munmap               \n      0.0          141,700         69       2,053.6      1,431.0      1,007       14,453       1,763.4  fclose               \n      0.0          123,084         16       7,692.8      5,635.5      1,063       22,553       6,244.1  write                \n      0.0          113,446          2      56,723.0     56,723.0     52,791       60,655       5,560.7  pthread_create       \n      0.0           80,095         15       5,339.7      4,115.0      1,789       15,904       3,756.7  open                 \n      0.0           29,997          5       5,999.4      3,918.0      1,410       16,372       5,916.5  fopen64              \n      0.0           25,575          2      12,787.5     12,787.5      9,492       16,083       4,660.5  socket               \n      0.0           13,661          5       2,732.2      1,858.0      1,045        6,703       2,274.3  pthread_cond_signal  \n      0.0           12,962          1      12,962.0     12,962.0     12,962       12,962           0.0  connect              \n      0.0           12,710          2       6,355.0      6,355.0      1,101       11,609       7,430.3  pthread_mutex_trylock\n      0.0           10,017          1      10,017.0     10,017.0     10,017       10,017           0.0  pipe2                \n      0.0            6,291          1       6,291.0      6,291.0      6,291        6,291           0.0  getc                 \n      0.0            4,856          1       4,856.0      4,856.0      4,856        4,856           0.0  fread                \n      0.0            3,791          3       1,263.7      1,188.0      1,062        1,541         248.3  fcntl                \n      0.0            3,692          3       1,230.7      1,150.0      1,078        1,464         205.3  fflush               \n      0.0            3,620          2       1,810.0      1,810.0      1,524        2,096         404.5  sigaction            \n      0.0            2,106          1       2,106.0      2,106.0      2,106        2,106           0.0  fputs_unlocked       \n      0.0            1,780          1       1,780.0      1,780.0      1,780        1,780           0.0  bind                 \n      0.0            1,637          1       1,637.0      1,637.0      1,637        1,637           0.0  fcntl64              \n      0.0            1,376          1       1,376.0      1,376.0      1,376        1,376           0.0  listen               \n\n[5/8] Executing 'cuda_api_sum' stats report\n\n Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)               Name            \n --------  ---------------  ---------  -----------  -----------  ---------  ----------  ------------  ----------------------------\n     64.9       73,600,176         10  7,360,017.6     25,209.5     13,686  29,189,994  12,030,081.2  cudaLaunchKernel            \n     19.6       22,201,745          9  2,466,860.6     11,600.0      3,985  15,341,866   5,310,374.7  cudaMemcpyAsync             \n     13.2       15,018,628          2  7,509,314.0  7,509,314.0  2,117,769  12,900,859   7,624,796.1  cudaFree                    \n      2.0        2,217,914         13    170,608.8    172,874.0      4,544     329,658      81,216.7  cudaMalloc                  \n      0.2          222,195      1,149        193.4        165.0         89       1,779         108.2  cuGetProcAddress_v2         \n      0.2          173,034          9     19,226.0      6,419.0      5,973      55,838      17,545.3  cudaStreamSynchronize       \n      0.0           25,834         18      1,435.2        371.0        354      15,734       3,605.2  cudaEventCreateWithFlags    \n      0.0           24,330         10      2,433.0      2,015.5      1,109       6,827       1,643.2  cudaStreamIsCapturing_v10000\n      0.0            4,335          3      1,445.0      1,419.0      1,209       1,707         250.0  cuInit                      \n      0.0            2,221          4        555.3        217.5        150       1,636         721.3  cuModuleGetLoadingMode      \n\n[6/8] Executing 'cuda_gpu_kern_sum' stats report\n\n Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                                                  Name                                                \n --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------------------------------------------------------------------------------------\n     57.5       11,818,611          4  2,954,652.8  1,712,425.5  1,701,017  6,692,743  2,492,068.0  volta_sgemm_128x64_tn                                                                               \n     28.4        5,834,459          1  5,834,459.0  5,834,459.0  5,834,459  5,834,459          0.0  volta_sgemm_128x64_nn                                                                               \n      5.8        1,197,893          1  1,197,893.0  1,197,893.0  1,197,893  1,197,893          0.0  void &lt;unnamed&gt;::softmax_warp_forward&lt;float, float, float, (int)10, (bool)0, (bool)0&gt;(T2 *, const T1…\n      3.7          763,694          1    763,694.0    763,694.0    763,694    763,694          0.0  void at::native::&lt;unnamed&gt;::vectorized_layer_norm_kernel&lt;float, float&gt;(int, T2, const T1 *, const T…\n      2.6          535,827          2    267,913.5    267,913.5    263,226    272,601      6,629.1  void at::native::vectorized_elementwise_kernel&lt;(int)4, at::native::AUnaryFunctor&lt;float, float, floa…\n      1.9          389,943          1    389,943.0    389,943.0    389,943    389,943          0.0  void at::native::vectorized_elementwise_kernel&lt;(int)4, at::native::CUDAFunctor_add&lt;float&gt;, at::deta…\n\n[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n\n Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)  Min (ns)  Max (ns)   StdDev (ns)           Operation          \n --------  ---------------  -----  ---------  --------  --------  ---------  -----------  ----------------------------\n    100.0        6,588,201      9  732,022.3     736.0       704  6,510,028  2,166,783.8  [CUDA memcpy Host-to-Device]\n\n[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n\n Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          \n ----------  -----  --------  --------  --------  --------  -----------  ----------------------------\n     33.560      9     3.729     0.001     0.001    32.768       10.890  [CUDA memcpy Host-to-Device]\n\nGenerated:\n    /content/report2.nsys-rep\n    /content/report2.sqlite\n\n\n(Numbers will differ slightly each time we run these cells)\nLooking at the “cuda_gpu_kern_sum” report, we notice:\n\nvolta_sgemm_128x64_tn (58.4% of GPU time, previously 61.9%):\n\nWe see a slight decrease in what is likely the matrix multiplication for computing attention weights. Though small on some tiny sample data, imagine these gains multiplied exponentially on real world training and inference involving text, images, video etc.\n\nvolta_sgemm_64x64_tn (13.3%, previously 14.2%):\n\nFinal matrix multiplication with the value matrix.\n\nvolta_sgemm_128x64_nn (10.6%, previously 11.3%):\n\nThe linear layer matrix multiplications.\n\nvectorized_layer_norm_kernel (7.7%, previously 8.1%):\n\nThis corresponds to the LayerNorm operation in the SimpleTransformer class.\n\nvectorized_elementwise_kernel (4.7% + 3.9% = 8.6%, previously 3.1%):\n\nThis now appears as two separate kernels, possibly for different elementwise operations.\n\nsoftmax_warp_forward (1.3%, previously 1.4%):\n\nThis is still the softmax operation applied to the attention weights.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Profiling and optimizing PyTorch training</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to GPUs and CUDA programming",
    "section": "",
    "text": "Preface\nThis short course aims to equip readers and participants with an understanding of GPU architecture and considerations for programming accelerated workloads effectively.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "6  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Summary</span>"
    ]
  }
]