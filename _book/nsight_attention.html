<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Profiling and optimizing PyTorch training – gpu_cuda_book</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./summary.html" rel="next">
<link href="./python_cuda.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./nsight_attention.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Profiling and optimizing PyTorch training</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">gpu_cuda_book</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gpu_architecture.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Getting to know the device and platform</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./c_to_cuda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Getting started with CUDA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./python_cuda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Python and CUDA</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nsight_attention.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Profiling and optimizing PyTorch training</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#install-nsight-tools" id="toc-install-nsight-tools" class="nav-link active" data-scroll-target="#install-nsight-tools"><span class="header-section-number">5.0.1</span> Install Nsight tools</a></li>
  <li><a href="#check-the-installation" id="toc-check-the-installation" class="nav-link" data-scroll-target="#check-the-installation"><span class="header-section-number">5.0.2</span> Check the installation</a></li>
  <li><a href="#simple-attention" id="toc-simple-attention" class="nav-link" data-scroll-target="#simple-attention"><span class="header-section-number">5.0.3</span> Simple attention</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Profiling and optimizing PyTorch training</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="https://colab.research.google.com/github/rastringer/GPU_CUDA_overview/blob/main/nsight_attention.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>
<div id="cell-1" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install jupyterlab<span class="op">-</span>nvidia<span class="op">-</span>nsight</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><em>(Make sure you are using the free T4 runtime in Colab)</em></p>
<p>Since using GPUs is the most expensive step in ML training and inference, no small amount of work goes into optimizing their use. In the real world, very few organizations and developers work on low-level kernel optimizations. They typically work further up the stack with frameworks such as PyTorch, leaving PyTorch’s optimizations to those working on its backend (which of course uses CUDA).</p>
<p>To give us a lens into the operations being performed on the accelerator and their efficiency in this scenario, there are a variety of profiling tools available. In this notebook, we will explore the use of Nvidia’s <a href="https://developer.nvidia.com/nsight-systems">Nsight</a>. The software is available as a desktop application and command line tool.</p>
<section id="install-nsight-tools" class="level3" data-number="5.0.1">
<h3 data-number="5.0.1" class="anchored" data-anchor-id="install-nsight-tools"><span class="header-section-number">5.0.1</span> Install Nsight tools</h3>
<p>Since Colabs are essentially a linux-based virtual machine, we can use <code>apt get</code> to install the Nvidia tools</p>
<div id="cell-5" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>bash</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>apt update</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>apt install <span class="op">-</span>y <span class="op">--</span>no<span class="op">-</span>install<span class="op">-</span>recommends gnupg</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>echo <span class="st">"deb http://developer.download.nvidia.com/devtools/repos/ubuntu$(source /etc/lsb-release; echo "</span>$DISTRIB_RELEASE<span class="st">" | tr -d .)/$(dpkg --print-architecture) /"</span> <span class="op">|</span> tee <span class="op">/</span>etc<span class="op">/</span>apt<span class="op">/</span>sources.<span class="bu">list</span>.d<span class="op">/</span>nvidia<span class="op">-</span>devtools.<span class="bu">list</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>apt<span class="op">-</span>key adv <span class="op">--</span>fetch<span class="op">-</span>keys http:<span class="op">//</span>developer.download.nvidia.com<span class="op">/</span>compute<span class="op">/</span>cuda<span class="op">/</span>repos<span class="op">/</span>ubuntu1804<span class="op">/</span>x86_64<span class="op">/</span><span class="dv">7</span><span class="er">fa2af80</span>.pub</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>apt update</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>apt install nsight<span class="op">-</span>systems<span class="op">-</span>cli</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="check-the-installation" class="level3" data-number="5.0.2">
<h3 data-number="5.0.2" class="anchored" data-anchor-id="check-the-installation"><span class="header-section-number">5.0.2</span> Check the installation</h3>
<div id="cell-7" class="cell" data-outputid="a56d2f25-2f77-429b-f814-c09943bbec2e" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>nsys status <span class="op">-</span>e</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Timestamp counter supported: Yes

CPU Profiling Environment Check
Root privilege: enabled
Linux Kernel Paranoid Level = 2
Linux Distribution = Ubuntu
Linux Kernel Version = 6.1.85+: OK
Linux perf_event_open syscall available: OK
Sampling trigger event available: OK
Intel(c) Last Branch Record support: Not Available
CPU Profiling Environment (process-tree): OK
CPU Profiling Environment (system-wide): OK

See the product documentation at https://docs.nvidia.com/nsight-systems for more information,
including information on how to set the Linux Kernel Paranoid Level.</code></pre>
</div>
</div>
</section>
<section id="simple-attention" class="level3" data-number="5.0.3">
<h3 data-number="5.0.3" class="anchored" data-anchor-id="simple-attention"><span class="header-section-number">5.0.3</span> Simple attention</h3>
<p>Here’s our basic attention mechanism that computes query, key, and value matrices to generate weighted representations of input data. The SimpleTransformer class combines this attention mechanism with layer normalization in a residual connection setup.</p>
<p>We will include profiling code to measure CPU and GPU performance metrics when running the model on sample input data.</p>
<div id="cell-9" class="cell" data-outputid="945ade2d-d03e-4ca6-b231-4957e3769c3c" data-execution_count="8">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile profiler.py</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleAttention(nn.Module):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> torch.matmul(q, k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        attn_weights <span class="op">=</span> torch.softmax(attn_weights, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(attn_weights, v)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleTransformer(nn.Module):</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, num_heads):</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> SimpleAttention(embed_dim)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        attn_output <span class="op">=</span> <span class="va">self</span>.attention(x)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.norm(x <span class="op">+</span> attn_output)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a model and sample input</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleTransformer(embed_dim, num_heads<span class="op">=</span><span class="dv">1</span>).cuda()</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(batch_size, seq_length, embed_dim).cuda()</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.cuda.profiler <span class="im">as</span> profiler</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Warm-up run</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>model(sample_input)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Profile the model</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> profiler.profile(activities<span class="op">=</span>[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA], record_shapes<span class="op">=</span><span class="va">True</span>, profile_memory<span class="op">=</span><span class="va">True</span>, with_stack<span class="op">=</span><span class="va">True</span>) <span class="im">as</span> prof:</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> profiler.record_function(<span class="st">"model_inference"</span>):</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        model(sample_input)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Print profiling results</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prof.key_averages().table(sort_by<span class="op">=</span><span class="st">"cuda_time_total"</span>, row_limit<span class="op">=</span><span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Writing profiler.py</code></pre>
</div>
</div>
<div id="cell-10" class="cell" data-outputid="7df6c90f-746d-4bb5-d98b-2a2719e1ae81" data-execution_count="10">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>nsys profile <span class="op">--</span>stats<span class="op">=</span>true python profiler.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Collecting data...
Traceback (most recent call last):
  File "/content/profiler.py", line 46, in &lt;module&gt;
    with profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA], record_shapes=True, profile_memory=True, with_stack=True) as prof:
AttributeError: module 'torch.cuda.profiler' has no attribute 'ProfilerActivity'
Generating '/tmp/nsys-report-4b28.qdstrm'
[1/8] [========================100%] report1.nsys-rep
[2/8] [========================100%] report1.sqlite
[3/8] Executing 'nvtx_sum' stats report
SKIPPED: /content/report1.sqlite does not contain NV Tools Extension (NVTX) data.
[4/8] Executing 'osrt_sum' stats report

 Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)    Min (ns)    Max (ns)    StdDev (ns)            Name         
 --------  ---------------  ---------  ------------  ------------  ---------  -----------  ------------  ----------------------
     76.0    1,702,896,538         29  58,720,570.3  77,869,201.0      4,006  100,150,826  44,495,375.6  poll                  
     13.2      295,663,488      1,672     176,832.2       3,329.0      1,004   15,374,049     641,970.2  read                  
      4.0       89,772,667        611     146,927.4      16,308.0      1,509   23,241,632   1,163,272.8  ioctl                 
      3.9       86,647,314      5,802      14,934.0       3,006.5      1,006   13,686,743     246,663.1  stat64                
      1.7       38,487,406        992      38,797.8      13,551.0      2,049      844,849     101,781.4  open64                
      0.5       10,831,409      7,221       1,500.0       1,469.0      1,000       18,716         610.5  lstat64               
      0.3        6,090,781          1   6,090,781.0   6,090,781.0  6,090,781    6,090,781           0.0  nanosleep             
      0.2        3,767,157         76      49,567.9      10,444.5      3,410    2,163,110     247,130.7  mmap64                
      0.2        3,716,694      1,853       2,005.8       1,794.0      1,018       25,280       1,277.1  fstat64               
      0.0          596,671          4     149,167.8      49,473.0     35,192      462,533     209,267.0  sem_timedwait         
      0.0          521,407         74       7,046.0       3,520.5      1,697      169,539      19,585.7  fopen                 
      0.0          333,517         20      16,675.8       9,374.5      2,751      105,415      22,298.1  mmap                  
      0.0          303,794         16      18,987.1      12,165.5      1,402       57,334      18,133.4  write                 
      0.0          261,847          8      32,730.9      32,683.5     23,171       39,984       5,418.0  fgets                 
      0.0          207,125          3      69,041.7      68,447.0     66,655       72,023       2,733.0  sleep                 
      0.0          142,461         70       2,035.2       1,419.5      1,079       13,265       1,673.8  fclose                
      0.0          118,808          2      59,404.0      59,404.0     56,211       62,597       4,515.6  pthread_create        
      0.0          114,648          9      12,738.7      13,780.0      7,881       21,544       4,325.4  munmap                
      0.0           92,736         12       7,728.0       4,024.0      1,623       27,163       8,106.5  pthread_cond_signal   
      0.0           85,302         15       5,686.8       4,267.0      2,028       22,918       5,157.5  open                  
      0.0           33,190          5       6,638.0       5,099.0      1,635       16,932       5,952.6  fopen64               
      0.0           23,712          2      11,856.0      11,856.0      8,025       15,687       5,417.9  socket                
      0.0           10,493          1      10,493.0      10,493.0     10,493       10,493           0.0  connect               
      0.0            8,915          1       8,915.0       8,915.0      8,915        8,915           0.0  pthread_cond_broadcast
      0.0            7,372          1       7,372.0       7,372.0      7,372        7,372           0.0  pipe2                 
      0.0            6,383          1       6,383.0       6,383.0      6,383        6,383           0.0  getc                  
      0.0            5,305          1       5,305.0       5,305.0      5,305        5,305           0.0  fread                 
      0.0            5,009          2       2,504.5       2,504.5      1,829        3,180         955.3  sigaction             
      0.0            4,778          4       1,194.5       1,195.0      1,043        1,345         123.3  fflush                
      0.0            3,201          3       1,067.0       1,055.0      1,033        1,113          41.3  fcntl                 
      0.0            2,580          1       2,580.0       2,580.0      2,580        2,580           0.0  fputs_unlocked        
      0.0            2,300          1       2,300.0       2,300.0      2,300        2,300           0.0  bind                  
      0.0            1,609          1       1,609.0       1,609.0      1,609        1,609           0.0  fcntl64               
      0.0            1,505          1       1,505.0       1,505.0      1,505        1,505           0.0  listen                
      0.0            1,318          1       1,318.0       1,318.0      1,318        1,318           0.0  pthread_mutex_trylock 

[5/8] Executing 'cuda_api_sum' stats report

 Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)    Min (ns)    Max (ns)   StdDev (ns)               Name            
 --------  ---------------  ---------  ------------  ------------  ---------  ----------  ------------  ----------------------------
     59.0      133,942,988          8  16,742,873.5      44,480.0     18,304  75,424,540  29,365,709.9  cudaLaunchKernel            
     19.9       45,123,527          9   5,013,725.2      21,450.0      7,039  44,149,269  14,677,890.0  cudaMemcpyAsync             
     19.1       43,452,758          2  21,726,379.0  21,726,379.0  5,346,866  38,105,892  23,164,129.4  cudaFree                    
      0.8        1,898,877          6     316,479.5     216,731.0     12,598     976,777     341,108.4  cudaMalloc                  
      0.6        1,421,966         18      78,998.1         668.0        616   1,401,568     330,071.6  cudaEventCreateWithFlags    
      0.3          658,905          3     219,635.0       3,212.0      2,603     653,090     375,383.2  cudaStreamIsCapturing_v10000
      0.2          469,330      1,149         408.5         256.0        126     146,038       4,301.4  cuGetProcAddress_v2         
      0.1          182,122          9      20,235.8       7,191.0      5,845      68,490      20,841.1  cudaStreamSynchronize       
      0.0            7,048          3       2,349.3       2,324.0      2,234       2,490         129.9  cuInit                      
      0.0            1,790          4         447.5         300.5        252         937         327.1  cuModuleGetLoadingMode      

[6/8] Executing 'cuda_gpu_kern_sum' stats report

 Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                                                  Name                                                
 --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  ----------------------------------------------------------------------------------------------------
     61.8          632,402          3  210,800.7  210,491.0   209,947   211,964      1,043.5  volta_sgemm_128x64_tn                                                                               
     14.3          146,205          1  146,205.0  146,205.0   146,205   146,205          0.0  volta_sgemm_64x64_tn                                                                                
     11.2          114,973          1  114,973.0  114,973.0   114,973   114,973          0.0  volta_sgemm_128x64_nn                                                                               
      8.1           82,590          1   82,590.0   82,590.0    82,590    82,590          0.0  void at::native::&lt;unnamed&gt;::vectorized_layer_norm_kernel&lt;float, float&gt;(int, T2, const T1 *, const T…
      3.1           32,127          1   32,127.0   32,127.0    32,127    32,127          0.0  void at::native::vectorized_elementwise_kernel&lt;(int)4, at::native::CUDAFunctor_add&lt;float&gt;, at::deta…
      1.4           14,240          1   14,240.0   14,240.0    14,240    14,240          0.0  void &lt;unnamed&gt;::softmax_warp_forward&lt;float, float, float, (int)7, (bool)0, (bool)0&gt;(T2 *, const T1 …

[7/8] Executing 'cuda_gpu_mem_time_sum' stats report

 Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)           Operation          
 --------  ---------------  -----  --------  --------  --------  --------  -----------  ----------------------------
    100.0          675,793      9  75,088.1     768.0       735   599,634    197,031.0  [CUDA memcpy Host-to-Device]

[8/8] Executing 'cuda_gpu_mem_size_sum' stats report

 Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          
 ----------  -----  --------  --------  --------  --------  -----------  ----------------------------
      4.068      9     0.452     0.001     0.001     3.277        1.067  [CUDA memcpy Host-to-Device]

Generated:
    /content/report1.nsys-rep
    /content/report1.sqlite</code></pre>
</div>
</div>
<p>(Numbers will differ slightly each time we run these cells)</p>
<p>Let’s analyze the “cuda_gpu_kern_sum” report, which shows the GPU kernel executions:</p>
<ul>
<li><p><code>volta_sgemm_128x64_tn</code> (61.9% of GPU time): This is likely the matrix multiplication for computing attention weights (q * k.transpose(-2, -1)). It’s using NVIDIA’s optimized GEMM (General Matrix Multiplication) kernel. Typically the most compute-intensive operation in a. transformer model.</p></li>
<li><p><code>volta_sgemm_64x64_tn</code> (14.2% of GPU time): This could be another part of the attention computation, possibly the final matrix multiplication with the value matrix (attn_weights * v).</p></li>
<li><p><code>volta_sgemm_128x64_nn</code> (11.3% of GPU time): This might be the matrix multiplication in one of the linear layers (query, key, or value projection).</p></li>
<li><p><code>vectorized_layer_norm_kernel</code> (8.1% of GPU time): This corresponds to the LayerNorm operation in the SimpleTransformer class. vectorized_elementwise_kernel (3.1% of GPU time): This could be the element-wise addition in the residual connection (x + attn_output).</p></li>
<li><p><code>softmax_warp_forward</code> (1.4% of GPU time): This is the softmax operation applied to the attention weights.</p></li>
</ul>
<p>The <code>SimpleAttention</code> class operations are primarily represented by items 1, 2, 3, and 6 in this list. These operations account for about 88.8% of the GPU kernel execution time, which indicates that the attention mechanism is indeed a significant part of the computation. To optimize this, we could:</p>
<ul>
<li>Use the optimized attention mechanism as suggested in the tutorial (torch.nn.functional.scaled_dot_product_attention).</li>
<li>Experiment with different batch sizes or sequence lengths to find the optimal configuration for your hardware.</li>
<li>Consider using mixed precision (float16).</li>
</ul>
<p>Let’s optimize our attention mechanism to use the <code>torch.nn.functional.scaled_dot_product_attention</code> function, optimized for GPUs. This method uses the Flash Attention algorithm when available.</p>
<div id="cell-13" class="cell" data-outputid="71ab40c1-543e-4c19-f1bd-1b55adf0c5e2" data-execution_count="11">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>writefile profiler.py</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OptimizedAttention(nn.Module):</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim):</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> embed_dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.scaled_dot_product_attention(q, k, v, scale<span class="op">=</span><span class="va">self</span>.scale)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the SimpleTransformer class to use OptimizedAttention</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OptimizedTransformer(nn.Module):</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, num_heads):</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> OptimizedAttention(embed_dim)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(embed_dim)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        attn_output <span class="op">=</span> <span class="va">self</span>.attention(x)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.norm(x <span class="op">+</span> attn_output)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a model and sample input</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new model with the optimized attention</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>optimized_model <span class="op">=</span> OptimizedTransformer(embed_dim, num_heads<span class="op">=</span><span class="dv">1</span>).cuda()</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(batch_size, seq_length, embed_dim).cuda()</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.cuda.profiler <span class="im">as</span> profiler</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Warm-up run</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>optimized_model(sample_input)</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Profile the optimized model</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> profiler.profile(activities<span class="op">=</span>[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA], record_shapes<span class="op">=</span><span class="va">True</span>, profile_memory<span class="op">=</span><span class="va">True</span>, with_stack<span class="op">=</span><span class="va">True</span>) <span class="im">as</span> prof:</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> profiler.record_function(<span class="st">"optimized_model_inference"</span>):</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>        optimized_model(sample_input)</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Print profiling results</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prof.key_averages().table(sort_by<span class="op">=</span><span class="st">"cuda_time_total"</span>, row_limit<span class="op">=</span><span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Overwriting profiler.py</code></pre>
</div>
</div>
<div id="cell-14" class="cell" data-outputid="0b3b926e-db30-43a5-de83-6ae13fb2585f" data-execution_count="12">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>nsys profile <span class="op">--</span>stats<span class="op">=</span>true python profiler.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Collecting data...
Traceback (most recent call last):
  File "/content/profiler.py", line 48, in &lt;module&gt;
    with profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA], record_shapes=True, profile_memory=True, with_stack=True) as prof:
AttributeError: module 'torch.cuda.profiler' has no attribute 'ProfilerActivity'
Generating '/tmp/nsys-report-ed31.qdstrm'
[1/8] [========================100%] report2.nsys-rep
[2/8] [========================100%] report2.sqlite
[3/8] Executing 'nvtx_sum' stats report
SKIPPED: /content/report2.sqlite does not contain NV Tools Extension (NVTX) data.
[4/8] Executing 'osrt_sum' stats report

 Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)    Max (ns)    StdDev (ns)           Name         
 --------  ---------------  ---------  ------------  -----------  ---------  -----------  ------------  ---------------------
     79.1      501,166,791         17  29,480,399.5  2,985,176.0      3,734  100,157,864  38,521,524.2  poll                 
     11.6       73,748,312        639     115,412.1     12,665.0      1,374   18,555,420   1,004,303.4  ioctl                
      2.9       18,137,686      5,796       3,129.3      2,490.0      1,000       47,046       2,214.1  stat64               
      1.6        9,881,832      6,366       1,552.3      1,470.0      1,000       21,190         685.9  lstat64              
      1.3        8,400,908      1,104       7,609.5      2,719.5      1,007      251,640      16,190.3  read                 
      1.2        7,379,916        992       7,439.4      6,844.5      1,883       28,734       2,950.8  open64               
      0.8        5,069,938          1   5,069,938.0  5,069,938.0  5,069,938    5,069,938           0.0  nanosleep            
      0.6        3,616,562         76      47,586.3     10,088.5      4,364    2,222,049     254,372.7  mmap64               
      0.5        3,406,279      1,852       1,839.2      1,636.5      1,070       49,459       1,738.7  fstat64              
      0.1          906,478          9     100,719.8     67,874.0     41,581      396,079     111,299.9  sem_timedwait        
      0.1          354,877         74       4,795.6      3,567.5      1,586       25,689       4,162.3  fopen                
      0.0          287,427         27      10,645.4      7,683.0      2,462       61,914      11,272.9  mmap                 
      0.0          252,649          8      31,581.1     32,863.0     12,821       39,628       8,314.4  fgets                
      0.0          221,124          3      73,708.0     71,618.0     68,171       81,335       6,826.3  sleep                
      0.0          157,808         16       9,863.0      8,608.0      4,446       27,809       5,358.3  munmap               
      0.0          141,700         69       2,053.6      1,431.0      1,007       14,453       1,763.4  fclose               
      0.0          123,084         16       7,692.8      5,635.5      1,063       22,553       6,244.1  write                
      0.0          113,446          2      56,723.0     56,723.0     52,791       60,655       5,560.7  pthread_create       
      0.0           80,095         15       5,339.7      4,115.0      1,789       15,904       3,756.7  open                 
      0.0           29,997          5       5,999.4      3,918.0      1,410       16,372       5,916.5  fopen64              
      0.0           25,575          2      12,787.5     12,787.5      9,492       16,083       4,660.5  socket               
      0.0           13,661          5       2,732.2      1,858.0      1,045        6,703       2,274.3  pthread_cond_signal  
      0.0           12,962          1      12,962.0     12,962.0     12,962       12,962           0.0  connect              
      0.0           12,710          2       6,355.0      6,355.0      1,101       11,609       7,430.3  pthread_mutex_trylock
      0.0           10,017          1      10,017.0     10,017.0     10,017       10,017           0.0  pipe2                
      0.0            6,291          1       6,291.0      6,291.0      6,291        6,291           0.0  getc                 
      0.0            4,856          1       4,856.0      4,856.0      4,856        4,856           0.0  fread                
      0.0            3,791          3       1,263.7      1,188.0      1,062        1,541         248.3  fcntl                
      0.0            3,692          3       1,230.7      1,150.0      1,078        1,464         205.3  fflush               
      0.0            3,620          2       1,810.0      1,810.0      1,524        2,096         404.5  sigaction            
      0.0            2,106          1       2,106.0      2,106.0      2,106        2,106           0.0  fputs_unlocked       
      0.0            1,780          1       1,780.0      1,780.0      1,780        1,780           0.0  bind                 
      0.0            1,637          1       1,637.0      1,637.0      1,637        1,637           0.0  fcntl64              
      0.0            1,376          1       1,376.0      1,376.0      1,376        1,376           0.0  listen               

[5/8] Executing 'cuda_api_sum' stats report

 Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)               Name            
 --------  ---------------  ---------  -----------  -----------  ---------  ----------  ------------  ----------------------------
     64.9       73,600,176         10  7,360,017.6     25,209.5     13,686  29,189,994  12,030,081.2  cudaLaunchKernel            
     19.6       22,201,745          9  2,466,860.6     11,600.0      3,985  15,341,866   5,310,374.7  cudaMemcpyAsync             
     13.2       15,018,628          2  7,509,314.0  7,509,314.0  2,117,769  12,900,859   7,624,796.1  cudaFree                    
      2.0        2,217,914         13    170,608.8    172,874.0      4,544     329,658      81,216.7  cudaMalloc                  
      0.2          222,195      1,149        193.4        165.0         89       1,779         108.2  cuGetProcAddress_v2         
      0.2          173,034          9     19,226.0      6,419.0      5,973      55,838      17,545.3  cudaStreamSynchronize       
      0.0           25,834         18      1,435.2        371.0        354      15,734       3,605.2  cudaEventCreateWithFlags    
      0.0           24,330         10      2,433.0      2,015.5      1,109       6,827       1,643.2  cudaStreamIsCapturing_v10000
      0.0            4,335          3      1,445.0      1,419.0      1,209       1,707         250.0  cuInit                      
      0.0            2,221          4        555.3        217.5        150       1,636         721.3  cuModuleGetLoadingMode      

[6/8] Executing 'cuda_gpu_kern_sum' stats report

 Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                                                  Name                                                
 --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------------------------------------------------------------------------------------
     57.5       11,818,611          4  2,954,652.8  1,712,425.5  1,701,017  6,692,743  2,492,068.0  volta_sgemm_128x64_tn                                                                               
     28.4        5,834,459          1  5,834,459.0  5,834,459.0  5,834,459  5,834,459          0.0  volta_sgemm_128x64_nn                                                                               
      5.8        1,197,893          1  1,197,893.0  1,197,893.0  1,197,893  1,197,893          0.0  void &lt;unnamed&gt;::softmax_warp_forward&lt;float, float, float, (int)10, (bool)0, (bool)0&gt;(T2 *, const T1…
      3.7          763,694          1    763,694.0    763,694.0    763,694    763,694          0.0  void at::native::&lt;unnamed&gt;::vectorized_layer_norm_kernel&lt;float, float&gt;(int, T2, const T1 *, const T…
      2.6          535,827          2    267,913.5    267,913.5    263,226    272,601      6,629.1  void at::native::vectorized_elementwise_kernel&lt;(int)4, at::native::AUnaryFunctor&lt;float, float, floa…
      1.9          389,943          1    389,943.0    389,943.0    389,943    389,943          0.0  void at::native::vectorized_elementwise_kernel&lt;(int)4, at::native::CUDAFunctor_add&lt;float&gt;, at::deta…

[7/8] Executing 'cuda_gpu_mem_time_sum' stats report

 Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)  Min (ns)  Max (ns)   StdDev (ns)           Operation          
 --------  ---------------  -----  ---------  --------  --------  ---------  -----------  ----------------------------
    100.0        6,588,201      9  732,022.3     736.0       704  6,510,028  2,166,783.8  [CUDA memcpy Host-to-Device]

[8/8] Executing 'cuda_gpu_mem_size_sum' stats report

 Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)           Operation          
 ----------  -----  --------  --------  --------  --------  -----------  ----------------------------
     33.560      9     3.729     0.001     0.001    32.768       10.890  [CUDA memcpy Host-to-Device]

Generated:
    /content/report2.nsys-rep
    /content/report2.sqlite</code></pre>
</div>
</div>
<p>(Numbers will differ slightly each time we run these cells)</p>
<p>Looking at the “cuda_gpu_kern_sum” report, we notice:</p>
<ul>
<li>volta_sgemm_128x64_tn (58.4% of GPU time, previously 61.9%):
<ul>
<li>We see a slight decrease in what is likely the matrix multiplication for computing attention weights. Though small on some tiny sample data, imagine these gains multiplied exponentially on real world training and inference involving text, images, video etc.</li>
</ul></li>
<li>volta_sgemm_64x64_tn (13.3%, previously 14.2%):
<ul>
<li>Final matrix multiplication with the value matrix.</li>
</ul></li>
<li>volta_sgemm_128x64_nn (10.6%, previously 11.3%):
<ul>
<li>The linear layer matrix multiplications.</li>
</ul></li>
<li>vectorized_layer_norm_kernel (7.7%, previously 8.1%):
<ul>
<li>This corresponds to the LayerNorm operation in the SimpleTransformer class.</li>
</ul></li>
<li>vectorized_elementwise_kernel (4.7% + 3.9% = 8.6%, previously 3.1%):
<ul>
<li>This now appears as two separate kernels, possibly for different elementwise operations.</li>
</ul></li>
<li>softmax_warp_forward (1.3%, previously 1.4%):
<ul>
<li>This is still the softmax operation applied to the attention weights.</li>
</ul></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./python_cuda.html" class="pagination-link" aria-label="Python and CUDA">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Python and CUDA</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./summary.html" class="pagination-link" aria-label="Summary">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Summary</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>